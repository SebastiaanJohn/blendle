{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blendle Topic & Event Extraction EN\n",
    "\n",
    "Based on workflows by:\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6\n",
    "\n",
    "https://towardsdatascience.com/natural-language-processing-event-extraction-f20d634661d3?gi=253736be2ed7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from tqdm import tqdm\n",
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and preprocessing data\n",
    "\n",
    "The following functions are used to load the JSON data in as a dataframe and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_json_data(path):\n",
    "    \"\"\"\n",
    "    import_json_data imports JSON data and keeps only the id, date,\n",
    "    headline and content of an article.\n",
    "\n",
    "    :path: path to raw JSON data\n",
    "    :return: list of dicts where each dict is a article.\n",
    "    \"\"\" \n",
    "    articles = []\n",
    "    data = [json.loads(line) for line in open(path, 'r')]\n",
    "    \n",
    "    for d in data:\n",
    "        article = {\n",
    "            'id' : d['id'],\n",
    "            'date' : d['date'],\n",
    "        }\n",
    "        for i in d['body']:\n",
    "            try:\n",
    "                if i['type'] == 'hl1':\n",
    "                    article['headline'] = i['content']\n",
    "            except:\n",
    "                break\n",
    "        content = []\n",
    "        for i in d['body']:\n",
    "            try:\n",
    "                if i['type'] == 'p':\n",
    "                    content.append(i['content'])\n",
    "            except:\n",
    "                continue\n",
    "        article['content'] = ' '.join(content)\n",
    "        articles.append(article)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    preprocess_data takes in list of dicts and returns\n",
    "    Pandas DataFrame with keys as columns and value\n",
    "    as rows, and removes HTML tags from text.\n",
    "\n",
    "    :data: list of dicts of articles.\n",
    "    \n",
    "    :return: dataframe.\n",
    "    \"\"\"\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    \n",
    "    dataframe = pd.DataFrame(data)\n",
    "    \n",
    "    # drop duplicate articles\n",
    "    dataframe = dataframe.drop_duplicates(subset='content', keep=\"last\")\n",
    "    \n",
    "    # remove HTML\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: TAG_RE.sub('', x))\n",
    "    dataframe['content'] = dataframe['content'].apply(lambda x: x.replace(\"&nbsp;\", \" \"))\n",
    "    \n",
    "    # create new article column\n",
    "    dataframe['article'] = dataframe['headline'] + ' ' + dataframe['content']\n",
    "    \n",
    "    # make date column datetime\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'])\n",
    "    \n",
    "    return dataframe.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(path):\n",
    "    \"\"\"\n",
    "    Loops over directory and calls above functions to\n",
    "    return dataframe.\n",
    "\n",
    "    :path: path to JSON files.\n",
    "    \n",
    "    :return: dataframe.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for file in tqdm(os.listdir(path)):\n",
    "        data = import_json_data(path + file)\n",
    "        frames.append(preprocess_data(data))\n",
    "    \n",
    "    return pd.concat(frames).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:22<00:00, 41.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bnl-newyorktimes528-20200216-489baa4a905a</td>\n",
       "      <td>2020-02-16 00:00:00+00:00</td>\n",
       "      <td>Questioning CPR as a Default Response</td>\n",
       "      <td>DR. MONIQUE STARKS DUKE UNIVERSITY SCHOOL OF M...</td>\n",
       "      <td>Questioning CPR as a Default Response DR. MONI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bnl-chicagotribune-20200219-89419fb1</td>\n",
       "      <td>2020-02-19 00:00:00+00:00</td>\n",
       "      <td>‘Taking Sexy Back’ a fantastic and timely book</td>\n",
       "      <td>If I didn’t know better, I would think Alexand...</td>\n",
       "      <td>‘Taking Sexy Back’ a fantastic and timely book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bnl-atavist-20200226-5e52a7d36b667</td>\n",
       "      <td>2020-02-26 00:00:00+00:00</td>\n",
       "      <td>Deliverance</td>\n",
       "      <td>Devilry of the kind necessary to kill a toddle...</td>\n",
       "      <td>Deliverance Devilry of the kind necessary to k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bnl-economist-20200131-7f87b05bd7e</td>\n",
       "      <td>2020-01-31 00:00:00+00:00</td>\n",
       "      <td>Whendunnit?</td>\n",
       "      <td>Since the first use of fingerprints to identif...</td>\n",
       "      <td>Whendunnit? Since the first use of fingerprint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bnl-fastcompany-20200201-ba11405d1ed</td>\n",
       "      <td>2020-02-01 00:00:00+00:00</td>\n",
       "      <td>EXPERIENCE MATTERS</td>\n",
       "      <td>IN DIGITAL With support from SAP, Fast Company...</td>\n",
       "      <td>EXPERIENCE MATTERS IN DIGITAL With support fro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id                      date  \\\n",
       "0  bnl-newyorktimes528-20200216-489baa4a905a 2020-02-16 00:00:00+00:00   \n",
       "1       bnl-chicagotribune-20200219-89419fb1 2020-02-19 00:00:00+00:00   \n",
       "2         bnl-atavist-20200226-5e52a7d36b667 2020-02-26 00:00:00+00:00   \n",
       "3         bnl-economist-20200131-7f87b05bd7e 2020-01-31 00:00:00+00:00   \n",
       "4       bnl-fastcompany-20200201-ba11405d1ed 2020-02-01 00:00:00+00:00   \n",
       "\n",
       "                                         headline  \\\n",
       "0           Questioning CPR as a Default Response   \n",
       "1  ‘Taking Sexy Back’ a fantastic and timely book   \n",
       "2                                     Deliverance   \n",
       "3                                     Whendunnit?   \n",
       "4                              EXPERIENCE MATTERS   \n",
       "\n",
       "                                             content  \\\n",
       "0  DR. MONIQUE STARKS DUKE UNIVERSITY SCHOOL OF M...   \n",
       "1  If I didn’t know better, I would think Alexand...   \n",
       "2  Devilry of the kind necessary to kill a toddle...   \n",
       "3  Since the first use of fingerprints to identif...   \n",
       "4  IN DIGITAL With support from SAP, Fast Company...   \n",
       "\n",
       "                                             article  \n",
       "0  Questioning CPR as a Default Response DR. MONI...  \n",
       "1  ‘Taking Sexy Back’ a fantastic and timely book...  \n",
       "2  Deliverance Devilry of the kind necessary to k...  \n",
       "3  Whendunnit? Since the first use of fingerprint...  \n",
       "4  EXPERIENCE MATTERS IN DIGITAL With support fro...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'data_en/'\n",
    "df = create_df(PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets checkout an article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Questioning CPR as a Default Response DR. MONIQUE STARKS DUKE UNIVERSITY SCHOOL OF MEDICINE A FEW MONTHS AGO, an ambulance brought a woman in her 90s to the emergency department at Brigham and Women’s Hospital in Boston. Her metastatic breast cancer had entered its final stages, and she had begun home hospice care. Yet a family member who had discovered her unresponsive that morning had called 911. The paramedics determined that she was in cardiac arrest, began cardiopulmonary resuscitation and put a breathing tube down her throat. “It’s a common scenario,” said Dr. Kei Ouchi, an emergency physician and researcher at Brigham and Women’s who reviews such cases. “And it’s not going to have a good outcome.” At the hospital, the patient’s blood pressure continued to fall despite intravenous medications. “She was trying to die, and it was only a matter of time before she arrested again,” Dr. Ouchi said. An oncologist and emergency physicians met with the patient’s family, and explained that her odds of survival were extremely low and that she might well suffer permanent cognitive damage even if she lived. The family agreed to stop resuscitation and return to comfort measures, and the woman died within 24 hours. Dr. Ouchi’s question: “Should CPR even have been started for this patient?” It’s a question arising with greater frequency as more people live to advanced ages, when the odds of surviving an out-ofhospital cardiac arrest after CPR are grim, and the chances of avoiding significant neurological disability are worse. “Many of us in daily practice have the perception that we regularly do resuscitations that are futile from the inception,” said Dr. Patrick Druwé, an intensive care physician at Ghent University Hospital in Belgium. “We wanted to examine that.” Dr. Druwé’s team organized a network to survey health care professionals in Europe, Japan, Israel and the United States. The group’s latest study, published in the Journal of the American Geriatrics Society, looke'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article'][0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are using BERT for our embeddings, we do not have lemmatize words or remove stop words. We can feed the raw text straight into the model! Now we convert the article column to a list for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 344740 articles.\n"
     ]
    }
   ],
   "source": [
    "# create list of articles to cluster model\n",
    "train = df.article.tolist()\n",
    "print(f'There are {len(train)} articles.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates BERT embeddings by encoding data.\n",
    "\n",
    "For more information see:\n",
    "https://huggingface.co/models\n",
    "\n",
    "We used the following models:\n",
    "\n",
    "* **English text**: 'distilbert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1307cfd750349ec8986cadb5ca5aceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(344740, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(train, show_progress_bar=True)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if using pre embedded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344740, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings = np.load('embeddings/embeddings_en.npy')\n",
    "# embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing dimensionality of embeddings.\n",
    "\n",
    "For more information see: https://umap-learn.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344740, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=5, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment if using pre embedded UMAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344740, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# umap_embeddings = np.load('embeddings/umap_embeddings_en.npy')\n",
    "# umap_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering articles into topics\n",
    "\n",
    "For more information see: https://github.com/scikit-learn-contrib/hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class based TF-IDF\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(\n",
    "        documents\n",
    "    )\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "\n",
    "\n",
    "# topic representation\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "\n",
    "    top_n_words = {\n",
    "        label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1]\n",
    "        for i, label in enumerate(labels)\n",
    "    }\n",
    "    top_n_words_clean = {\n",
    "        label: [words[j] for j in indices[i]][::-1] for i, label in enumerate(labels)\n",
    "    }\n",
    "\n",
    "    return top_n_words, top_n_words_clean\n",
    "\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (\n",
    "        df.groupby([\"Topic\"])\n",
    "        .Doc.count()\n",
    "        .reset_index()\n",
    "        .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis=\"columns\")\n",
    "        .sort_values(\"Size\", ascending=False)\n",
    "    )\n",
    "\n",
    "    return topic_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>241045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>152</td>\n",
       "      <td>13492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>111</td>\n",
       "      <td>7511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>664</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>85</td>\n",
       "      <td>1868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>537</td>\n",
       "      <td>1863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>114</td>\n",
       "      <td>1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>725</td>\n",
       "      <td>1598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>577</td>\n",
       "      <td>1551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>463</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic    Size\n",
       "0       -1  241045\n",
       "153    152   13492\n",
       "112    111    7511\n",
       "665    664    2163\n",
       "86      85    1868\n",
       "538    537    1863\n",
       "115    114    1701\n",
       "726    725    1598\n",
       "578    577    1551\n",
       "464    463    1454"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df = pd.DataFrame(train, columns=[\"Doc\"])\n",
    "docs_df[\"Topic\"] = cluster.labels_\n",
    "docs_df[\"Doc_ID\"] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby([\"Topic\"], as_index=False).agg({\"Doc\": \" \".join})\n",
    "\n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(train))\n",
    "\n",
    "top_n_words, top_n_words_clean = extract_top_n_words_per_topic(\n",
    "    tf_idf, count, docs_per_topic, n=20\n",
    ")\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 740 topics.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(topic_sizes)} topics.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check out different keywords relating to a topic using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sanders', 0.014172281729564824),\n",
       " ('biden', 0.01199950858886089),\n",
       " ('voters', 0.011383020763193545),\n",
       " ('candidates', 0.009810150507078074),\n",
       " ('iowa', 0.009126619806508533),\n",
       " ('democratic', 0.00881395758274148),\n",
       " ('buttigieg', 0.007657650558769603),\n",
       " ('democrats', 0.007099756541263775),\n",
       " ('warren', 0.006998057847040383),\n",
       " ('primary', 0.006820382291034903),\n",
       " ('polls', 0.0067843080403029825),\n",
       " ('candidate', 0.006755786074205527),\n",
       " ('race', 0.00667055535337568),\n",
       " ('senator', 0.00632638094080744),\n",
       " ('poll', 0.005758337215866537),\n",
       " ('republican', 0.005714212024029153),\n",
       " ('campaign', 0.005582795841044771),\n",
       " ('debate', 0.005372495115494521),\n",
       " ('presidential', 0.005089820825376859),\n",
       " ('republicans', 0.004729822072244175)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_words[725]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following function to get all topics relating to a specific word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_number(topics, word):\n",
    "    \"\"\"\n",
    "    Returns all topics numbers given a specific word\n",
    "\n",
    "    :topics: list of dicts of topics.\n",
    "    :word: the topic you're looking for (str) \n",
    "    \n",
    "    :return: list of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    keys = len(topics.keys())\n",
    "    \n",
    "    for topic in topics.items():\n",
    "        if word in topic[1]:\n",
    "            results.append(topic)\n",
    "    \n",
    "    if len(results) == 0: \n",
    "        return 'no topic found'\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(355,\n",
       "  ['______',\n",
       "   'briefing',\n",
       "   'brexit',\n",
       "   'parliament',\n",
       "   'smarter',\n",
       "   'boris',\n",
       "   'prime',\n",
       "   'johnson',\n",
       "   'minister',\n",
       "   'josephson',\n",
       "   'break',\n",
       "   'youto',\n",
       "   'snapshot',\n",
       "   'britain',\n",
       "   'eleanor',\n",
       "   'today',\n",
       "   'clue',\n",
       "   'puzzles',\n",
       "   'nytimes',\n",
       "   'morning']),\n",
       " (577,\n",
       "  ['brexit',\n",
       "   'johnson',\n",
       "   'britain',\n",
       "   'parliament',\n",
       "   'eu',\n",
       "   'european',\n",
       "   'labour',\n",
       "   'mrs',\n",
       "   'deal',\n",
       "   'corbyn',\n",
       "   'union',\n",
       "   'ireland',\n",
       "   'prime',\n",
       "   'referendum',\n",
       "   'minister',\n",
       "   'british',\n",
       "   'lawmakers',\n",
       "   'bloc',\n",
       "   'boris',\n",
       "   'conservative'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic_number(top_n_words_clean, 'brexit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event extraction\n",
    "We add the embeddings to the dataframe so we can extract one event every day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['keywords'] = df['Topic'].apply(lambda x: top_n_words_clean[x])\n",
    "    df['keywords_prob'] = df['Topic'].apply(lambda x: top_n_words[x])\n",
    "    df['embeddings'] = list(umap_embeddings)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_dataframes(df1, df2):\n",
    "    df = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    clean_df = df[['id', 'date', 'Topic', 'keywords', 'headline', 'content', 'embeddings']]\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the first and last dataframes to get a new dataframe that has all the necessary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>Topic</th>\n",
       "      <th>keywords</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bnl-newyorktimes528-20200216-489baa4a905a</td>\n",
       "      <td>2020-02-16 00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>[pandemic, biden, workers, money, social, comp...</td>\n",
       "      <td>Questioning CPR as a Default Response</td>\n",
       "      <td>DR. MONIQUE STARKS DUKE UNIVERSITY SCHOOL OF M...</td>\n",
       "      <td>[8.258087, 5.4001455, 8.514114, 4.913037, 3.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bnl-chicagotribune-20200219-89419fb1</td>\n",
       "      <td>2020-02-19 00:00:00+00:00</td>\n",
       "      <td>537</td>\n",
       "      <td>[novel, book, books, writing, story, literary,...</td>\n",
       "      <td>‘Taking Sexy Back’ a fantastic and timely book</td>\n",
       "      <td>If I didn’t know better, I would think Alexand...</td>\n",
       "      <td>[8.350002, 5.7192316, 6.9386177, 4.987291, 1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bnl-atavist-20200226-5e52a7d36b667</td>\n",
       "      <td>2020-02-26 00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>[pandemic, biden, workers, money, social, comp...</td>\n",
       "      <td>Deliverance</td>\n",
       "      <td>Devilry of the kind necessary to kill a toddle...</td>\n",
       "      <td>[8.8728895, 6.339035, 7.4803505, 3.258, 4.066124]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bnl-economist-20200131-7f87b05bd7e</td>\n",
       "      <td>2020-01-31 00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>[pandemic, biden, workers, money, social, comp...</td>\n",
       "      <td>Whendunnit?</td>\n",
       "      <td>Since the first use of fingerprints to identif...</td>\n",
       "      <td>[8.657768, 4.904919, 7.007837, 4.775185, 4.732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bnl-fastcompany-20200201-ba11405d1ed</td>\n",
       "      <td>2020-02-01 00:00:00+00:00</td>\n",
       "      <td>-1</td>\n",
       "      <td>[pandemic, biden, workers, money, social, comp...</td>\n",
       "      <td>EXPERIENCE MATTERS</td>\n",
       "      <td>IN DIGITAL With support from SAP, Fast Company...</td>\n",
       "      <td>[8.226043, 4.2056518, 6.258909, 5.8530326, 4.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id                      date  Topic  \\\n",
       "0  bnl-newyorktimes528-20200216-489baa4a905a 2020-02-16 00:00:00+00:00     -1   \n",
       "1       bnl-chicagotribune-20200219-89419fb1 2020-02-19 00:00:00+00:00    537   \n",
       "2         bnl-atavist-20200226-5e52a7d36b667 2020-02-26 00:00:00+00:00     -1   \n",
       "3         bnl-economist-20200131-7f87b05bd7e 2020-01-31 00:00:00+00:00     -1   \n",
       "4       bnl-fastcompany-20200201-ba11405d1ed 2020-02-01 00:00:00+00:00     -1   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [pandemic, biden, workers, money, social, comp...   \n",
       "1  [novel, book, books, writing, story, literary,...   \n",
       "2  [pandemic, biden, workers, money, social, comp...   \n",
       "3  [pandemic, biden, workers, money, social, comp...   \n",
       "4  [pandemic, biden, workers, money, social, comp...   \n",
       "\n",
       "                                         headline  \\\n",
       "0           Questioning CPR as a Default Response   \n",
       "1  ‘Taking Sexy Back’ a fantastic and timely book   \n",
       "2                                     Deliverance   \n",
       "3                                     Whendunnit?   \n",
       "4                              EXPERIENCE MATTERS   \n",
       "\n",
       "                                             content  \\\n",
       "0  DR. MONIQUE STARKS DUKE UNIVERSITY SCHOOL OF M...   \n",
       "1  If I didn’t know better, I would think Alexand...   \n",
       "2  Devilry of the kind necessary to kill a toddle...   \n",
       "3  Since the first use of fingerprints to identif...   \n",
       "4  IN DIGITAL With support from SAP, Fast Company...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [8.258087, 5.4001455, 8.514114, 4.913037, 3.66...  \n",
       "1  [8.350002, 5.7192316, 6.9386177, 4.987291, 1.9...  \n",
       "2  [8.8728895, 6.339035, 7.4803505, 3.258, 4.066124]  \n",
       "3  [8.657768, 4.904919, 7.007837, 4.775185, 4.732...  \n",
       "4  [8.226043, 4.2056518, 6.258909, 5.8530326, 4.2...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_df = add_embeddings(docs_df)\n",
    "clean_df = merge_dataframes(embedded_df, df)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_central_vector(dataframe, n=1):\n",
    "    \"\"\"\n",
    "    Extracts key event per day of dataframe of topic\n",
    "\n",
    "    :dataframe: Pandas DataFrame of specific topic number \n",
    "    :n: how many articles one a day are needed for something to be classified as a event (int)\n",
    "    \n",
    "    :return: new dataframe\n",
    "    \"\"\"\n",
    "    result = dataframe.iloc[0:0]\n",
    "    articles = dataframe.set_index('date')\n",
    "    \n",
    "    for name, group in articles.groupby(pd.Grouper(freq='D')): \n",
    "        if len(group.embeddings) > max(n - 1, n):\n",
    "            mean_vector = np.mean(group.embeddings)\n",
    "            em = np.matrix(group.embeddings.tolist())\n",
    "            index = pairwise_distances_argmin_min(mean_vector.reshape(1, -1), em)[0][0]\n",
    "            result = result.append(group.iloc[index])\n",
    "        elif len(group.embeddings) == 0:\n",
    "            continue\n",
    "        elif n == 1:\n",
    "            result = result.append(group)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_df(dataframe, topic):\n",
    "    \"\"\"\n",
    "    Returns new dataframe based on topic number\n",
    "\n",
    "    :dataframe: Pandas DataFrame\n",
    "    :topic: number (int)\n",
    "    \n",
    "    :return: new dataframe\n",
    "    \"\"\"\n",
    "    return dataframe.loc[dataframe['Topic'] == topic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summerize_article(dataframe, n_sentences=3):\n",
    "    \"\"\"\n",
    "    Creates new dataframe column with summarize of articles\n",
    "\n",
    "    :dataframe: Pandas DataFrame\n",
    "    :n_sentences: number of sentences the summary should be\n",
    "    \n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    model = Summarizer()\n",
    "    \n",
    "    dataframe['summary'] = dataframe['content'].apply(lambda x: model(x, num_sentences=n_sentences))\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeline(dataframe, topic, filename, summarize=True, n=1):\n",
    "    \"\"\"\n",
    "    Prints 1 headline of article everyday.\n",
    "\n",
    "    :dataframe: Pandas DataFrame\n",
    "    :topic: number (int)\n",
    "    :filename: output txt file name\n",
    "    :summarize: set to false if you don't want a \n",
    "                summarization of the content of an article\n",
    "    \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    df1 = get_topic_df(clean_df, topic)\n",
    "    df2 = get_central_vector(df1, n)\n",
    "    if summarize:\n",
    "        df3 = summerize_article(df2)\n",
    "        with open(filename, 'w') as f:\n",
    "            for article in tqdm(df3.itertuples()):\n",
    "                f.write(f'Date: {article.Index} \\nHeadline: {article.headline} \\nSummary: {article.summary} \\n\\n')\n",
    "    else:\n",
    "        with open(filename, 'w') as f:\n",
    "            for article in tqdm(df2.itertuples()):\n",
    "                f.write(f'Date: {article.Index} \\nHeadline: {article.headline} \\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa9e6d4e1d44525b9ba5bb35e5086a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956ece62b9374e96ba63ca5cd192abe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca8f0bfa67346028f86182ce778c3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "410it [00:00, 75104.36it/s]\n"
     ]
    }
   ],
   "source": [
    "create_timeline(clean_df, 577, 'brexit.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 2020 US Elections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "526it [00:00, 249740.08it/s]\n"
     ]
    }
   ],
   "source": [
    "create_timeline(clean_df, 725, 'elections.txt', summarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
